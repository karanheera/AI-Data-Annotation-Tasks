# Named Entity Recognition (NER) Annotation

This subproject demonstrates **skills in Named Entity Recognition (NER) data annotation** using **[Label Studio](https://labelstud.io/)** and **CoNLL format exports**, with added steps for **visualization**, **evaluation**, and **report generation**.

> ‚öôÔ∏è No model training is performed ‚Äî this project focuses purely on the **annotation process**, **data quality**, and **visual analysis** of labeled entities.

---

## üìÅ Folder Structure

```plaintext
annotations/
‚îî‚îÄ‚îÄ train.conll             # Labeled data exported from Label Studio (CoNLL format)

data/
‚îú‚îÄ‚îÄ GUIDELINES.md           # Annotation guidelines for labeling text data
‚îî‚îÄ‚îÄ social_media_texts.csv  # Randomly generated text samples for annotation

notebooks/                  
‚îî‚îÄ‚îÄ ner_annotations.ipynb   # Main notebook: visualize, evaluate & generate report

results/
‚îú‚îÄ‚îÄ annotation_stats.txt     # Entity frequency counts
‚îú‚îÄ‚îÄ eval_summary.txt         # Annotation evaluation summary
‚îú‚îÄ‚îÄ report.md                # GitHub-ready summary report
‚îî‚îÄ‚îÄ screenshot_output.png    # Screenshot of annotated entity visualization

README.md                    # (You are here)
```

---


## üìÇ Data Folder

The `data/` folder contains source text files and the annotation guidelines:

```plaintext
data/
‚îú‚îÄ‚îÄ GUIDELINES.md
‚îî‚îÄ‚îÄ social_media_texts.csv
````

> ‚ö†Ô∏è **Note:**
>
> * The text samples in `social_media_texts.csv` were **generated by giving prompts to ChatGPT** to create random social media-style texts suitable for NER annotation.
> * 100 of these raw samples were then imported into **Label Studio** and **annotated manually**.
> * `GUIDELINES.md` provides **instructions and best practices for annotators**, including entity definitions, BIO tagging rules, edge cases, and usage recommendations for Label Studio.


---

## Tools Used

| Component            | Purpose                                                  |
| -------------------- | -------------------------------------------------------- |
| **Label Studio**     | Manual NER annotation                                    |
| **CoNLL Format**     | Standard structured data export (token + label per line) |
| **spaCy + displaCy** | Entity visualization and inline rendering                |
| **Python + Jupyter** | For processing, analysis, and reporting                  |

---

## How to Run

1. **Navigate to the project folder**

   ```bash
   cd <project-folder>
   ```

2. **Install dependencies**

   ```bash
   pip install spacy jupyter
   ```

3. **Open the notebook**

   ```bash
   jupyter notebook notebooks/ner_annotations.ipynb
   ```

4. **Run all cells**

   * Reads `annotations/train.conll` (exported from Label Studio)
   * Visualizes entities inline in the notebook
   * Generates:

     * `results/annotation_stats.txt`
     * `results/eval_summary.txt`
     * `results/report.md`

---

## About the Annotations

All annotations were created in **Label Studio**, using the NER interface to tag entities such as:

* `B-PERSON`, `I-PERSON`
* `B-LOC`, `I-LOC`
* `B-ORG`, `I-ORG`
* `B-DATE`, `I-DATE`
* `B-TIME`, `I-TIME`
* `B-PRODUCT`, `I-PRODUCT`
* `B-EVENT`, `I-EVENT`
* `B-HASHTAG`, `B-URL`, `I-URL`

The annotated data was exported from Label Studio as **CoNLL format**, which follows a ‚Äútoken-per-line‚Äù structure.

> **Annotation skill highlight:** 100 text samples were annotated manually in **approximately 1 hour**, demonstrating speed, accuracy, and familiarity with NER labeling workflows.

---

## Before & After Example

| Stage                        | Description                                   | Example                                                                                                                                    |
| ---------------------------- | --------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
| **Before Annotation**        | Raw text                                      | `Apple released the iPhone 15 in California.`                                                                                              |
| **After Annotation (CoNLL)** | Token-labeled data exported from Label Studio | <pre>Apple      B-ORG<br>released  O<br>the       O<br>iPhone    B-PRODUCT<br>15        I-PRODUCT<br>in        O<br>California B-LOC</pre> |
| **Visualization**            | Inline visualization via spaCy `displaCy`     | ![NER Visualization](results/screenshot_output.png) <br>*Screenshot of annotated entities generated using spaCy displaCy*                  |

---

## Example Evaluation Summary

**`results/eval_summary.txt`:**

```plaintext
Total sentences: 100
Sentences with entities: 100
Total tokens: 881
Annotated tokens: 320 (36.32%)

Entity counts:
    B-ORG: 33
    B-HASHTAG: 1
    B-LOC: 32
    B-DATE: 19
    B-URL: 8
    I-URL: 48
    B-PERSON: 17
    B-PRODUCT: 27
    I-PRODUCT: 32
    I-PERSON: 7
    B-EVENT: 22
    I-EVENT: 17
    I-DATE: 8
    I-ORG: 4
    B-TIME: 13
    I-LOC: 26
    I-TIME: 6

BIO consistency errors: 0
```

---

## Generated Report

After running the notebook, `results/report.md` is automatically generated. It contains:

* Dataset statistics
* Entity count tables
* BIO tagging consistency check
* Inline visualization notes

**Example snippet from `report.md`:**

```markdown
# Named Entity Recognition Annotation Report

| Entity Label | Count |
|--------------|-------|
| B-DATE       | 19    |
| B-EVENT      | 22    |
| B-HASHTAG    | 1     |
| B-LOC        | 32    |
| B-ORG        | 33    |
| B-PERSON     | 17    |
| B-PRODUCT    | 27    |
| B-TIME       | 13    |
| B-URL        | 8     |
| I-DATE       | 8     |
| I-EVENT      | 17    |
| I-LOC        | 26    |
| I-ORG        | 4     |
| I-PERSON     | 7     |
| I-PRODUCT    | 32    |
| I-TIME       | 6     |
| I-URL        | 48    |
```

---

## Technologies Used

* **Python 3.9+**
* **Label Studio** ‚Äî for manual NER annotation
* **spaCy** ‚Äî for visualization and displaCy rendering
* **Jupyter Notebook** ‚Äî for report generation and analysis

---

## Author

**Karan Heera**

üåê [GitHub](https://github.com/karanheera/) ‚Ä¢ [LinkedIn](https://linkedin.com/in/karanheera/)

---

## üèÅ Summary

This NER Annotation demonstrates:

* Practical **NER dataset creation** with **Label Studio.**
* Ability to **analyze and visualize annotations** with spaCy.
* **Automated evaluation and reporting** for annotation quality.
* Strong understanding of **data structure and labeling consistency.**
* **Efficient annotation workflow**
* Inline **visualization of annotated entities** for reporting and validation.
* Use of **ChatGPT-generated random social media text samples** for annotation practice.